{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "404e16b0-a9d4-4343-b623-187653f31376",
   "metadata": {},
   "source": [
    "In our original proposal, we set out to create a Hangman game that a human could play, a basic rule-based bot, or a more advanced AI agent. The main objective was to compare how different strategies would solve the same word-guessing challenge. The bot was designed to guess letters based on a fixed ranking of English letter frequencies, while the AI would use statistical modeling and pattern recognition to make more intelligent guesses. To support this, we trained the AI using a dataset of English words with associated frequencies, enabling it to predict likely letters based on partial word patterns and word length. We also aimed to make the game replayable with detailed statistics tracking, allowing us to evaluate performance across thousands of simulations. The final implementation successfully supports all three modes of play, and our testing framework was able to simulate 50,000 games, revealing that the AI significantly outperformed the rule-based bot in both accuracy and consistency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e351fbb6-2b26-418c-bc1d-c997b02cc0ea",
   "metadata": {},
   "source": [
    "To support our AI Hangman system, we used two datasets. The primary dataset was Rachael Tatman’s English Word Frequency list, which includes words from the English language along with their usage frequency, sourced from the Google Web Trillion Word Corpus. This dataset contains a single key feature: the frequency count for each word, which we used to prioritize more common words during AI guessing. To complement this, we constructed a second dataset ourselves by analyzing the characters in the word list. Using a Python counter, we extracted three key features: Total Occurrences, Word Occurrences, and Letter Frequency. Total Occurrences measures how often each letter appears across the entire vocabulary. Word Occurrences counts how many individual words contain each letter (ignoring duplicates within a word). Finally, Letter Frequency was calculated by dividing the total occurrences of each letter by the total number of letters, giving us a normalized percentage. Frequency-based and pattern-based guessing strategies were both essential to our AI Hangman system because they addressed different stages of uncertainty during gameplay. For instance, frequency-based guessing helped early on by prioritizing common letters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d396578-fd49-4295-b173-3837832a5bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words:\n",
    "    letter_total.update(word)\n",
    "    letter_word_occurrence.update(set(word))  # Only count each letter once per word\n",
    "\n",
    "# Step 3: Total number of letters (for percentages)\n",
    "total_letters = sum(letter_total.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e097cab-d430-4e1f-a177-6bac284a2e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "Letter,Occurrences,Word_Occurrences,Frequency\n",
    "a,227927,173719,9.153279389328919\n",
    "b,55191,51338,2.2164054402350417\n",
    "c,105207,91557,4.224988986443587\n",
    "d,84511,75907,3.39386204561801\n",
    "e,261655,187226,10.50775607372035..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed81f85b-a9a8-4cc4-af6d-ca2e6e5e7dda",
   "metadata": {},
   "source": [
    "The AI player in this Hangman implementation uses a hybrid strategy that combines pattern matching with probabilistic reasoning based on real-world word frequency data. During a preprocessing step, the AI analyzes a large collection of English words and constructs letter frequency distributions for each word length. These distributions serve as a statistical model indicating which letters are most likely to appear in words of a given length. During gameplay, the AI filters the word list to find candidates that match the current known letter pattern (e.g., _ a _ _ e) while excluding previously guessed incorrect letters. If any matching words are found, the AI calculates the weighted frequency of each remaining unguessed letter across these words and selects the most likely one. If no matches are found, it defaults to using the precomputed frequency distribution for that word length. This approach allows the AI to adapt to both the structural constraints of the game and the statistical likelihood of letters, significantly improving its guessing accuracy. In extensive testing with 50,000 games, the AI achieved a 91% win rate, outperforming a simpler bot that relied solely on static letter frequency, which achieved an 80% win rate. This demonstrates that the AI's combination of learned distributions and real-time pattern matching leads to more effective gameplay and better handling of edge cases and rare words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a229ea6c-e57d-4154-809e-8abe53331d0a",
   "metadata": {},
   "source": [
    "In our AI Hangman implementation, several tunable parameters function similarly to hyperparameters and directly influence performance. First, we designed a word weighting formula that prioritizes words based on their frequency in a large corpus. This formula adjusts raw frequencies with a scaling factor and additional rules for the top 1,000 most common words. Tuning this formula would allow the AI to either emphasize highly frequent words more aggressively or distribute weight more evenly across the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab8e76b-a79e-458e-ad96-4a602dd140b1",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Normalize relative to the lowest freq you found manually\n",
    "    weight = raw_freq / 12711\n",
    "\n",
    "    # Penalize high-frequency (common) words\n",
    "    if i < 30:\n",
    "        weight = (i / 1000)  # Make them very low\n",
    "    elif i < 1000:\n",
    "        weight *= (i / 1000)  # Gradual reduction for semi-common words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0c51e3-a5e3-4991-8a08-a8ac54645d4e",
   "metadata": {},
   "source": [
    "Another important parameter is the minimum word length, currently set to exclude words shorter than three letters. This helps eliminate overly simple cases, but could be adjusted to include more edge scenarios or tailor difficulty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717d4388-1039-40b7-98bd-711e3c6dbd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if word.isalpha() and len(word) >= 3 and checkWordContainsVowel(word):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c084d59-c7b6-44d9-9199-e1d83fda38d9",
   "metadata": {},
   "source": [
    "Additionally, the letter distribution vectors used in our AI are generated by analyzing character frequencies across words of the same length. The smoothing and normalization of these vectors affect how the AI prioritizes certain letters when guessing, and they could be improved by adjusting how letter counts are scaled or balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7b9eb1-1635-4d72-9d16-06af9a5544c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    for length, ctr in length_freq.items():\n",
    "        total = sum(ctr.values())\n",
    "        vec = torch.zeros(26)\n",
    "        for letter, cnt in ctr.items():\n",
    "            vec[ord(letter) - ord('a')] = cnt / total\n",
    "        dist[length] = vec\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059be943-85d9-4835-a03f-b7ed8354e6aa",
   "metadata": {},
   "source": [
    "Lastly, the guess strategy switch point for the bot—currently set to activate pattern-based guessing when only two attempts remain—is another tunable choice. Adjusting this threshold could significantly change the bot's behavior, making it more cautious or aggressive depending on the game state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9aacec4-cb93-46dc-827c-9fd530b7abda",
   "metadata": {},
   "outputs": [],
   "source": [
    "guess = get_best_letter_from_likely_word(word_completion, guessed_letters, words, frequencies) \\\n",
    "        if attempts_remaining <= 2 else get_bot_guess(guessed_letters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bba4f0-3ca9-4d00-9a8f-80554f7f3c9a",
   "metadata": {},
   "source": [
    "While we accomplished most of our core objectives, there were several planned features we did not have time to implement, most notably enabling the AI to learn across multiple games by adapting its strategy based on past outcomes. Currently, the AI uses the same statistical model in every game without adjusting its decision-making based on previous wins or losses. With more time, we would build a reinforcement learning loop or feedback system that tracks statistics such as success rates of guessed letters, frequent misclassifications, and word structures that tend to cause failure. This would allow the AI to gradually shift its guessing preferences, dynamically adjust its weighting strategy, and improve mid-game accuracy by blending contextual probabilities with pattern matching. Another improvement we would consider is implementing a seed-based word selection method, allowing us to generate identical game scenarios for both the bot and the AI. By ensuring both models face the same sequence of words and game conditions, we could make more accurate and controlled comparisons between their performances. This would eliminate variability caused by random word selection and allow for more rigorous evaluation of which strategy performs better under identical circumstances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b6f2be-209e-4c60-af8b-4ebd72b12d6b",
   "metadata": {},
   "source": [
    "The AI significantly outperformed the bot, with the AI winning 45,409 out of 50,000 games, resulting in a win rate of 90.82%, while the bot won 40,108 games, with a win rate of 80.22%. The goal was for the system to achieve an 80% win rate, which we exceeded, showing the potential for improvement with additional game rounds and learning opportunities. In terms of insights gained from analyzing frequency-based guessing, we found that the bot, relying solely on static letter frequency, struggled with words that had irregular letter patterns. The AI, however, was much more adaptable, using learned distributions and contextual clues to handle such cases more effectively. One lesson from the AI’s performance was the importance of having a dynamic learning process that evolves over time. While the static approach of the bot provided decent performance, the AI’s ability to adjust based on the partial word patterns during gameplay contributed to its superior win rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143516fe-0f17-4bb4-ba69-05d55fbf1b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a28aeb7-33b6-41db-a5a4-4f9d3faa1647",
   "metadata": {},
   "source": [
    "In conclusion, this project demonstrated the effectiveness of using AI and frequency analysis to improve the performance of a simple word-guessing game. While we didn’t have time to implement certain advanced features, the current implementation shows the potential for even better performance with additional time and learning from multiple games. Future work could involve integrating reinforcement learning, adding neural network models, and creating a feedback loop to further enhance the AI's adaptability.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
